# ÄÃ¢y lÃ  project Ã¡p dá»¥ng torch.compile vÃ  ONNX Ä‘á»ƒ tá»‘i Æ°u hiá»‡u suáº¥t cá»§a mÃ´ hÃ¬nh TTS.

> Sáº½ cÃ³ 2 pháº§n 1 lÃ  vá» torch.compile, 2 lÃ  vá» ONNX vÃ  cuá»‘i cÃ¹ng lÃ  so sÃ¡nh hiá»‡u nÄƒng cá»§a cáº£ 2.

# Tá»•ng quan vá» `torch.compile`

## Tham sá»‘ cá»§a hÃ m `torch.compile`

```python
torch.compile(
    model: Callable[[_InputT], _RetT],
    *,
    fullgraph: bool = False,
    dynamic: Optional[bool] = None,
    backend: Union[str, Callable] = 'inductor',
    mode: Optional[str] = None,
    options: Optional[Dict[str, Union[str, int, bool]]] = None,
    disable: bool = False
) â†’ Callable[[_InputT], _RetT]
```

### Giáº£i thÃ­ch chi tiáº¿t cÃ¡c tham sá»‘:

- **`model` (Callable)**  
  HÃ m hoáº·c module mÃ  báº¡n muá»‘n tá»‘i Æ°u hÃ³a báº±ng cÃ¡ch biÃªn dá»‹ch.

- **`fullgraph` (bool)**  
  - Máº·c Ä‘á»‹nh lÃ  `False`: PyTorch sáº½ cá»‘ gáº¯ng tÃ¬m cÃ¡c pháº§n trong model cÃ³ thá»ƒ Ä‘Æ°á»£c biÃªn dá»‹ch.  
  - Náº¿u Ä‘áº·t lÃ  `True`: yÃªu cáº§u toÃ n bá»™ hÃ m pháº£i Ä‘Æ°á»£c biá»ƒu diá»…n thÃ nh má»™t biá»ƒu Ä‘á»“ duy nháº¥t. Náº¿u khÃ´ng thá»ƒ (cÃ³ "graph breaks"), sáº½ bÃ¡o lá»—i.

- **`dynamic` (bool hoáº·c None)**  
  - `True`: sá»­ dá»¥ng dynamic shape tracing Ä‘á»ƒ trÃ¡nh pháº£i biÃªn dá»‹ch láº¡i khi kÃ­ch thÆ°á»›c Ä‘áº§u vÃ o thay Ä‘á»•i.  
  - `False`: luÃ´n luÃ´n chuyÃªn biá»‡t hÃ³a (specialize), khÃ´ng sinh kernel Ä‘á»™ng.  
  - `None` (máº·c Ä‘á»‹nh): tá»± Ä‘á»™ng phÃ¡t hiá»‡n thay Ä‘á»•i shape vÃ  biÃªn dá»‹ch láº¡i khi cáº§n thiáº¿t.

- **`backend` (str hoáº·c Callable)**  
  - Backend Ä‘á»ƒ thá»±c hiá»‡n biÃªn dá»‹ch. Máº·c Ä‘á»‹nh lÃ  `"inductor"` â€“ cÃ¢n báº±ng tá»‘t giá»¯a hiá»‡u nÄƒng vÃ  chi phÃ­.  
  - DÃ¹ng `torch._dynamo.list_backends()` Ä‘á»ƒ xem cÃ¡c backend khÃ´ng thá»­ nghiá»‡m.  
  - DÃ¹ng `torch._dynamo.list_backends(None)` Ä‘á»ƒ xem cáº£ backend thá»­ nghiá»‡m/debug.  
  - CÃ³ thá»ƒ Ä‘Äƒng kÃ½ backend tuá»³ chá»‰nh theo hÆ°á»›ng dáº«n táº¡i:  
    https://pytorch.org/docs/main/torch.compiler_custom_backends.html#registering-custom-backends

- **`mode` (str)**  
  Má»™t sá»‘ cháº¿ Ä‘á»™ cÃ³ thá»ƒ sá»­ dá»¥ng:
  - `"default"`: cháº¿ Ä‘á»™ máº·c Ä‘á»‹nh, cÃ¢n báº±ng giá»¯a hiá»‡u nÄƒng vÃ  overhead.
  - `"reduce-overhead"`: giáº£m chi phÃ­ váº­n hÃ nh Python, há»¯u Ã­ch vá»›i batch nhá», sá»­ dá»¥ng CUDA Graphs.
  - `"max-autotune"`: kÃ­ch hoáº¡t tá»‘i Ä‘a kháº£ nÄƒng autotuning (dÃ¹ng Triton, CUDA graphs náº¿u cÃ³).
  - `"max-autotune-no-cudagraphs"`: nhÆ° trÃªn nhÆ°ng khÃ´ng sá»­ dá»¥ng CUDA graphs.

  CÃ³ thá»ƒ dÃ¹ng `torch._inductor.list_mode_options()` Ä‘á»ƒ xem cá»¥ thá»ƒ cÃ¡c cáº¥u hÃ¬nh tÆ°Æ¡ng á»©ng vá»›i tá»«ng mode.

- **`options` (dict)**  
  Tuá»³ chá»n nÃ¢ng cao cho backend. Má»™t vÃ i option Ä‘Ã¡ng chÃº Ã½:
  - `epilogue_fusion`: gá»™p cÃ¡c phÃ©p toÃ¡n pointwise, cáº§n káº¿t há»£p vá»›i `max_autotune`.
  - `max_autotune`: cho phÃ©p profiling Ä‘á»ƒ chá»n cáº¥u hÃ¬nh tá»‘t nháº¥t cho cÃ¡c phÃ©p nhÃ¢n ma tráº­n.
  - `fallback_random`: há»— trá»£ debug cÃ¡c váº¥n Ä‘á» vá» Ä‘á»™ chÃ­nh xÃ¡c.
  - `shape_padding`: pad thÃªm kÃ­ch thÆ°á»›c tensor Ä‘á»ƒ tá»‘i Æ°u truy xuáº¥t bá»™ nhá»› GPU.
  - `triton.cudagraphs`: giáº£m overhead cá»§a Python khi dÃ¹ng CUDA.
  - `trace.enabled`: flag há»— trá»£ debug hiá»‡u quáº£.
  - `trace.graph_diagram`: hiá»ƒn thá»‹ sÆ¡ Ä‘á»“ biá»ƒu Ä‘á»“ sau khi thá»±c hiá»‡n fusion.

  Äá»ƒ xem danh sÃ¡ch Ä‘áº§y Ä‘á»§ cÃ¡c option há»— trá»£, dÃ¹ng `torch._inductor.list_options()`.

- **`disable` (bool)**  
  Náº¿u Ä‘áº·t `True`, `torch.compile()` sáº½ khÃ´ng lÃ m gÃ¬ cáº£ â€” phÃ¹ há»£p khi cáº§n táº¯t compile táº¡m thá»i Ä‘á»ƒ test.

## VÃ­ dá»¥ sá»­ dá»¥ng

```python
@torch.compile(options={"triton.cudagraphs": True}, fullgraph=True)
def foo(x):
    return torch.sin(x) + torch.cos(x)
```

## Má»™t vÃ i lÆ°u Ã½ sau khi thá»­ nghiá»‡m thá»±c táº¿

Theo nhÆ° testing em Ä‘Ã£ lÃ m thÃ¬ viá»‡c sá»­ dá»¥ng `torch.compile` cáº§n Ä‘Æ°á»£c cÃ¢n nháº¯c ká»¹, khÃ´ng thá»ƒ "compile" má»™t model má»™t cÃ¡ch mÃ¡y mÃ³c.  

Cá»¥ thá»ƒ:
- `torch.compile` chá»‰ hoáº¡t Ä‘á»™ng hiá»‡u quáº£ vá»›i pháº§n **tÃ­nh toÃ¡n thuáº§n tÃºy**.
- Nhá»¯ng pháº§n liÃªn quan Ä‘áº¿n I/O (vÃ­ dá»¥: Ä‘á»c file, ghi log, print, hoáº·c cÃ¡c thao tÃ¡c khÃ´ng thuáº§n tensor) cáº§n Ä‘Æ°á»£c tÃ¡ch **ra khá»i hÃ m `forward`** hoáº·c khÃ´ng nÃªn náº±m trong pháº¡m vi biÃªn dá»‹ch.
- Khi sá»­ dá»¥ng sai cÃ¡ch, PyTorch sáº½ khÃ´ng thá»ƒ dá»±ng Ä‘Æ°á»£c biá»ƒu Ä‘á»“ hoáº·c hiá»‡u nÄƒng sáº½ khÃ´ng nhÆ° mong Ä‘á»£i.

## Liá»‡u sá»­ dá»¥ng `torch.compile` cÃ³ guarantee vá» viá»‡c improve performance khi sá»­ dá»¥ng?

CÃ¢u tráº£ lá»i lÃ  **cÃ³** vÃ  **khÃ´ng**.

Cá»¥ thá»ƒ thÃ¬ vá»›i **single batch inference**, cÃ³ nhá»¯ng trÆ°á»ng há»£p sáº½ cháº¡y nhanh hÆ¡n, nhÆ°ng cÅ©ng cÃ³ nhá»¯ng trÆ°á»ng há»£p hiá»‡u nÄƒng sáº½ kÃ©m hÆ¡n. Tuy nhiÃªn, vá»›i **multi-batch inference** (inference nhiá»u láº§n vá»›i cÃ¹ng má»™t model), theo káº¿t quáº£ thá»­ nghiá»‡m cá»§a em thÃ¬ **Ã­t nhiá»u sáº½ cÃ³ cáº£i thiá»‡n hiá»‡u nÄƒng sau khi compile**. Äiá»u nÃ y cÃ³ thá»ƒ lÃ½ giáº£i lÃ  do cÆ¡ cháº¿ caching vÃ  tá»‘i Æ°u hoÃ¡ kernel chá»‰ phÃ¡t huy hiá»‡u quáº£ sau má»™t vÃ i láº§n cháº¡y Ä‘áº§u tiÃªn.

## Táº¡i sao vá»›i mÃ´ hÃ¬nh TTS chá»‰ cÃ³ thá»ƒ compile vÃ o `g_net` thay vÃ¬ compile nhÆ° cÃ¡ch em Ä‘Ã£ lÃ m?

Sau khi tÃ¬m hiá»ƒu láº¡i thÃ¬ em nháº­n ra mÃ¬nh Ä‘Ã£ sá»­ dá»¥ng `torch.compile` chÆ°a Ä‘Ãºng cÃ¡ch. DÃ¹ vá» máº·t ká»¹ thuáº­t thÃ¬ viá»‡c compile toÃ n bá»™ mÃ´ hÃ¬nh váº«n cháº¡y Ä‘Æ°á»£c, nhÆ°ng hiá»‡u nÄƒng thá»±c táº¿ láº¡i **khÃ´ng á»•n Ä‘á»‹nh**, vÃ  **Ä‘a sá»‘ trÆ°á»ng há»£p cÃ²n cháº­m hÆ¡n cáº£ khi khÃ´ng dÃ¹ng compile**.

LÃ½ do chÃ­nh lÃ :
- Trong mÃ´ hÃ¬nh TTS, khÃ´ng pháº£i toÃ n bá»™ pipeline Ä‘á»u mang tÃ­nh "tÃ­nh toÃ¡n thuáº§n tensor" (pure tensor computation). CÃ³ ráº¥t nhiá»u thao tÃ¡c liÃªn quan Ä‘áº¿n I/O (nhÆ° xá»­ lÃ½ vÄƒn báº£n, thao tÃ¡c vá»›i waveform, tiá»n xá»­ lÃ½ hoáº·c háº­u xá»­ lÃ½), nhá»¯ng pháº§n nÃ y **khÃ´ng phÃ¹ há»£p Ä‘á»ƒ compile**, hoáº·c **lÃ m giÃ¡n Ä‘oáº¡n graph**, dáº«n Ä‘áº¿n máº¥t tá»‘i Æ°u.
- Khi compile toÃ n bá»™ pipeline, PyTorch sáº½ khÃ³ tá»‘i Æ°u do cÃ³ quÃ¡ nhiá»u pháº§n khÃ´ng thá»ƒ biá»ƒu diá»…n thÃ nh graph, dáº«n Ä‘áº¿n viá»‡c táº¡o ra nhiá»u **"graph break"** vÃ  **recompilation khÃ´ng cáº§n thiáº¿t**.
- Trong khi Ä‘Ã³, `g_net` lÃ  pháº§n model tÃ­nh toÃ¡n chÃ­nh, thÆ°á»ng lÃ  má»™t máº¡ng neural network thuáº§n tuÃ½. Viá»‡c compile riÃªng pháº§n nÃ y sáº½:
  - Giáº£m chi phÃ­ tracing láº¡i graph.
  - Táº­n dá»¥ng tá»‘t nháº¥t kháº£ nÄƒng tá»‘i Æ°u kernel.
  - KhÃ´ng bá»‹ áº£nh hÆ°á»Ÿng bá»Ÿi cÃ¡c thao tÃ¡c I/O khÃ¡c.

=> VÃ¬ váº­y, **viá»‡c compile chá»‰ riÃªng `g_net` lÃ  cÃ¡ch tá»‘i Æ°u nháº¥t**, Ä‘áº£m báº£o táº­n dá»¥ng Ä‘Æ°á»£c hiá»‡u nÄƒng cá»§a `torch.compile` mÃ  khÃ´ng áº£nh hÆ°á»Ÿng Ä‘áº¿n pháº§n cÃ²n láº¡i cá»§a pipeline.

## Sá»± khÃ¡c biá»‡t giá»¯a **mode** vÃ  **backend** trong `torch.compile()`

Hai tham sá»‘ `mode` vÃ  `backend` Ä‘á»u áº£nh hÆ°á»Ÿng Ä‘áº¿n cÃ¡ch PyTorch tá»‘i Æ°u mÃ´ hÃ¬nh khi sá»­ dá»¥ng `torch.compile`, nhÆ°ng vai trÃ² vÃ  má»©c Ä‘á»™ kiá»ƒm soÃ¡t cá»§a chÃºng lÃ  **khÃ¡c nhau**.

### 1. `backend` â€“ *Tham sá»‘ nÃ y lÃ  kiá»ƒu lá»±a chá»n bá»™ mÃ¡y Ä‘á»ƒ biÃªn dá»‹ch code*

- ÄÃ¢y lÃ  thÃ nh pháº§n chÃ­nh xá»­ lÃ½ viá»‡c **chuyá»ƒn Ä‘á»•i model sang dáº¡ng tá»‘i Æ°u hÃ³a** vÃ  thá»±c thi chÃºng.
- Hiá»ƒu Ä‘Æ¡n giáº£n thÃ¬ `backend` giá»‘ng nhÆ° **"cÃ´ng cá»¥" hoáº·c "engine"** Ä‘á»©ng sau viá»‡c biÃªn dá»‹ch vÃ  thá»±c thi mÃ´ hÃ¬nh.
- Máº·c Ä‘á»‹nh lÃ  `"inductor"` â€“ má»™t backend do PyTorch phÃ¡t triá»ƒn, tá»‘i Æ°u tá»‘t cho GPU/CPU.
- CÃ³ thá»ƒ thay báº±ng cÃ¡c backend khÃ¡c (vÃ­ dá»¥: `"onnxrt"` hoáº·c backend tÃ¹y chá»‰nh).

VÃ­ dá»¥:
```python
torch.compile(model, backend='inductor')
```

> Vá»›i thá»­ nghiá»‡m cá»§a em thÃ¬ viá»‡c thay Ä‘á»•i giá»¯a cÃ¡c backend em khÃ´ng thá»±c sá»± nháº­n tháº¥y sá»± khÃ¡c nhau láº¯m (cÃ³ thá»ƒ lÃ  vÃ¬ chá»‰ thá»±c hiá»‡n infer trÃªn 7 samples)

### 2. `mode` â€“ *HÆ°á»›ng tá»‘i Æ°u*

- `mode` xÃ¡c Ä‘á»‹nh **chiáº¿n lÆ°á»£c tá»‘i Æ°u hÃ³a** Ä‘Æ°á»£c Ã¡p dá»¥ng trong quÃ¡ trÃ¬nh compile.
- ÄÃ¢y giá»‘ng nhÆ° **"preset cáº¥u hÃ¬nh"** cho backend â€“ giÃºp báº¡n chá»n giá»¯a viá»‡c Æ°u tiÃªn tá»‘c Ä‘á»™, bá»™ nhá»›, hay autotuning.
- Má»—i mode sáº½ set cÃ¡c flag ná»™i bá»™ khÃ¡c nhau trong backend (thÆ°á»ng lÃ  inductor).

CÃ¡c mode phá»• biáº¿n:
- `"default"`: cÃ¢n báº±ng hiá»‡u nÄƒng vÃ  Ä‘á»™ á»•n Ä‘á»‹nh, phÃ¹ há»£p cho háº§u háº¿t trÆ°á»ng há»£p.
- `"reduce-overhead"`: giáº£m chi phÃ­ thá»±c thi Python, thÃ­ch há»£p cho batch nhá» (dÃ¹ng CUDA graphs).
- `"max-autotune"`: báº­t autotune tá»‘i Ä‘a, tÃ¬m cáº¥u hÃ¬nh tá»‘t nháº¥t cho cÃ¡c phÃ©p toÃ¡n nhÆ° matmul/convolution.
- `"max-autotune-no-cudagraphs"`: nhÆ° trÃªn, nhÆ°ng khÃ´ng dÃ¹ng CUDA graphs (phÃ¹ há»£p vá»›i má»™t sá»‘ constraint Ä‘áº·c biá»‡t).

VÃ­ dá»¥:
```python
torch.compile(model, mode='max-autotune')
```

> TÆ°Æ¡ng tá»± nhÆ° `backend`, em khÃ´ng tháº¥y cÃ³ sá»± khÃ¡c nhau rÃµ rÃ ng

## Vá» viá»‡c thá»­ nghiá»‡m compile xong hiá»‡u nÄƒng cÃ²n kÃ©m hÆ¡n

Viá»‡c nÃ y "Ä‘iÃªu" tháº­t, chÃ­nh vÃ¬ em Ä‘Ã£ cÃ³ giáº£i thÃ­ch á»Ÿ trÃªn, Ä‘Ã³ lÃ  em lÃ m sai

## bert-vits chuyá»ƒn tá»« `.pt` sang `.safetensors`, lÃ½ do lÃ  gÃ¬?

Theo nhÆ° research cá»§a em thÃ¬ lÃ½ do chÃ­nh náº±m á»Ÿ **tÃ­nh an toÃ n vÃ  tá»‘c Ä‘á»™ táº£i mÃ´ hÃ¬nh**.

### 1. `.safetensors` giÃºp **trÃ¡nh cÃ¡c váº¥n Ä‘á» báº£o máº­t**:

- File `.pt` (hoáº·c `.pth`) thÆ°á»ng Ä‘Æ°á»£c lÆ°u báº±ng `torch.save()`, mÃ  báº£n cháº¥t lÃ  sá»­ dá»¥ng `pickle` â€“ cÆ¡ cháº¿ nÃ y cho phÃ©p thá»±c thi **mÃ£ Python tÃ¹y Ã½** khi load láº¡i model (`torch.load()`).
- Äiá»u nÃ y tiá»m áº©n rá»§i ro báº£o máº­t, Ä‘áº·c biá»‡t khi load mÃ´ hÃ¬nh tá»« nguá»“n khÃ´ng tin cáº­y: ai Ä‘Ã³ cÃ³ thá»ƒ nhÃºng mÃ£ Ä‘á»™c vÃ  nÃ³ sáº½ **tá»± Ä‘á»™ng cháº¡y khi load mÃ´ hÃ¬nh**.
- Trong khi Ä‘Ã³, `.safetensors` lÃ  Ä‘á»‹nh dáº¡ng **hoÃ n toÃ n khÃ´ng thá»±c thi code**, chá»‰ lÆ°u dá»¯ liá»‡u tensor thuáº§n tÃºy, vÃ¬ váº­y loáº¡i bá» kháº£ nÄƒng chÃ¨n mÃ£ Ä‘á»™c.

### 2. `.safetensors` cÃ³ tá»‘c Ä‘á»™ load **nhanh hÆ¡n**:

- File `.safetensors` Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ **cho phÃ©p memory mapping** vÃ  truy cáº­p song song, giÃºp tÄƒng tá»‘c Ä‘á»™ load mÃ´ hÃ¬nh, Ä‘áº·c biá»‡t lÃ  khi dÃ¹ng trÃªn GPU.
- Cáº¥u trÃºc cá»§a Ä‘á»‹nh dáº¡ng nÃ y giÃºp trÃ¡nh viá»‡c giáº£i nÃ©n tá»«ng pháº§n nhÆ° `.pt`, do Ä‘Ã³ tiáº¿t kiá»‡m thá»i gian khá»Ÿi táº¡o model.

### 3. Há»— trá»£ rá»™ng trong cá»™ng Ä‘á»“ng hiá»‡n táº¡i:

- CÃ¡c dá»± Ã¡n lá»›n nhÆ° Hugging Face Transformers Ä‘Ã£ **máº·c Ä‘á»‹nh há»— trá»£ vÃ  khuyáº¿n nghá»‹ dÃ¹ng `.safetensors`** cho cÃ¡c model pre-trained.
- Vá»›i update gáº§n Ä‘Ã¢y cá»§a Pytoch (2.6.0 onward) thÃ¬ khi sá»­ dá»¥ng `torch.load()`, tham sá»‘ `weights_only` Ä‘Ã£ Ä‘Æ°á»£c set thÃ nh **True** máº·c Ä‘á»‹nh thay vÃ¬ **False** (load cáº£ code) nhÆ° trÆ°á»›c Ä‘Ã¢y.
- Vá»›i cÃ¡c mÃ´ hÃ¬nh nhÆ° `bert-vits`, viá»‡c chuyá»ƒn sang `.safetensors` cÅ©ng giÃºp Ä‘á»“ng bá»™ tá»‘t hÆ¡n vá»›i cÃ¡c pipeline hiá»‡n Ä‘áº¡i, dá»… dÃ ng chia sáº» mÃ  váº«n Ä‘áº£m báº£o an toÃ n.

## LÆ°u Ã½ khi sá»­ dá»¥ng `torch.compile()`

### `torch.compile()` **chá»‰ há»— trá»£ GPU cÃ³ compute capability â‰¥ 7.0**

- `torch.compile()` táº­n dá»¥ng cÃ¡c cÃ´ng nghá»‡ nhÆ° **Triton**, **Inductor**, vÃ  cÃ¡c ká»¹ thuáº­t kernel fusion hiá»‡n Ä‘áº¡i, vá»‘n Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ hoáº¡t Ä‘á»™ng hiá»‡u quáº£ nháº¥t trÃªn kiáº¿n trÃºc GPU tá»« **Volta** trá»Ÿ lÃªn (tá»©c lÃ  compute capability â‰¥ 7.0).
- Náº¿u dÃ¹ng GPU vá»›i compute capability tháº¥p hÆ¡n sáº½ khÃ´ng thá»ƒ compile.

### CÃ¡ch kiá»ƒm tra compute capability cá»§a GPU:

CÃ³ thá»ƒ cháº¡y lá»‡nh sau trong Python:

```python
import torch
print(torch.cuda.get_device_name(0))
print(torch.cuda.get_device_capability(0))
```


# Tá»•ng quan vá» **ONNX**

## ONNX lÃ  gÃ¬?

**ONNX** (Open Neural Network Exchange) lÃ  má»™t Ä‘á»‹nh dáº¡ng trung gian dÃ¹ng Ä‘á»ƒ **xuáº¥t vÃ  chia sáº» mÃ´ hÃ¬nh há»c sÃ¢u** giá»¯a cÃ¡c framework khÃ¡c nhau (vÃ­ dá»¥: PyTorch, TensorFlow, ONNX Runtime, v.v).

NÃ³ cho phÃ©p báº¡n **xuáº¥t model tá»« PyTorch**, sau Ä‘Ã³ **cháº¡y inference báº±ng ONNX Runtime**, thÆ°á»ng sáº½ tá»‘i Æ°u hÃ³a hiá»‡u suáº¥t hÆ¡n nhá» engine inference nháº¹, Ä‘áº·c biá»‡t trong mÃ´i trÆ°á»ng production hoáº·c mobile.

---

## CÃ¡c bÆ°á»›c chÃ­nh Ä‘á»ƒ sá»­ dá»¥ng ONNX trong PyTorch

### 1. Export mÃ´ hÃ¬nh sang ONNX

```python
torch.onnx.export(
    model,                      # MÃ´ hÃ¬nh PyTorch
    sample_input,               # Má»™t batch input máº«u Ä‘á»ƒ trace
    "model.onnx",               # TÃªn file Ä‘áº§u ra
    input_names=["input"],      # (Optional) TÃªn tensor Ä‘áº§u vÃ o
    output_names=["output"],    # (Optional) TÃªn tensor Ä‘áº§u ra
    dynamic_axes={              # (Optional) Cho phÃ©p kÃ­ch thÆ°á»›c Ä‘á»™ng
        "input": {0: "batch_size"},
        "output": {0: "batch_size"}
    },
    opset_version=17,           # PhiÃªn báº£n opset ONNX
    do_constant_folding=True,   # Tá»‘i Æ°u biá»ƒu thá»©c háº±ng
    export_params=True          # LÆ°u trá»ng sá»‘ vÃ o file ONNX
)
```

> ğŸ¯ **LÆ°u Ã½:** `opset_version` nÃªn lÃ  17 trá»Ÿ lÃªn Ä‘á»ƒ Ä‘áº£m báº£o tÆ°Æ¡ng thÃ­ch tá»‘t vá»›i ONNX Runtime má»›i.

#### Hoáº·c cÃ³ thá»ƒ dÃ¹ng Optimum-CLI nhÆ° vÃ­ dá»¥ sau Ä‘Ã¢y

```bash
pip install optimum[exporters]
```
Set --model dÃ¹ng Ä‘á»ƒ export model cá»§a Pytorch hoáº·c TensorFlow
```bash
optimum-cli export onnx --model distilbert/distilbert-base-uncased-distilled-squad distilbert_base_uncased_squad_onnx/
```
---

### 2. Load vÃ  cháº¡y inference báº±ng ONNX Runtime

```python
import onnxruntime as ort
import numpy as np

session = ort.InferenceSession("model.onnx")
inputs = {"input": input_tensor.numpy()}
outputs = session.run(None, inputs)
```

### 3. LÆ°u Ã½ vá» sá»± khÃ¡c biá»‡t giá»¯a **ONNX** vÃ  **torch.compile()** khi compile

Vá»›i mÃ´ hÃ¬nh TTS, náº¿u dÃ¹ng **ONNX**, **khÃ´ng thá»ƒ export cáº£ model má»™t cÃ¡ch trá»±c tiáº¿p**. Váº­y nÃªn thÆ°á»ng pháº£i **chia nhá» ra**, export riÃªng tá»«ng pháº§n nhÆ° `encode`, `decode`,... thÃ¬ má»›i cháº¡y Ä‘Æ°á»£c â€” khÃ¡ phiá»n vÃ  dá»… phÃ¡t sinh lá»—i.

Trong khi Ä‘Ã³, vá»›i **`torch.compile()`**, chá»‰ cáº§n **táº¡o má»™t wrapper cho `forward()`** chá»©a pháº§n tÃ­nh toÃ¡n chÃ­nh, miá»…n lÃ  khÃ´ng cÃ³ thao tÃ¡c I/O hoáº·c logic phá»©c táº¡p. Váº­y lÃ  xong, compile Ä‘Æ°á»£c ngay. **Dá»… hÆ¡n** so vá»›i viá»‡c pháº£i chia nhá» Ä‘á»ƒ export ONNX.

---

## So sÃ¡nh hiá»‡u nÄƒng: `torch.compile` vs ONNX

DÆ°á»›i Ä‘Ã¢y lÃ  káº¿t quáº£ thá»­ nghiá»‡m vá»›i mÃ´ hÃ¬nh Text-to-Speech (TTS), Ä‘Æ°á»£c Ä‘o theo Ä‘á»™ dÃ i vÄƒn báº£n:

| Äá»™ dÃ i vÄƒn báº£n | Sá»‘ kÃ½ tá»± TB | `torch.compile` Time | ONNX Time | Tá»‘c Ä‘á»™ (`compiled` vs `ONNX`) |
|----------------|-------------|-----------------------|-----------|-------------------------------|
| Very Short     | 3.0         | 0.17s                 | 0.33s     | ğŸŸ¢ `torch.compile` nhanh hÆ¡n 1.89x |
| Short          | 12.2        | 0.18s                 | 0.82s     | ğŸŸ¢ Nhanh hÆ¡n 4.61x              |
| Medium         | 34.0        | 0.26s                 | 2.10s     | ğŸŸ¢ Nhanh hÆ¡n 7.98x              |
| Long           | 91.7        | 0.61s                 | 6.15s     | ğŸŸ¢ Nhanh hÆ¡n 10.12x             |
| Very Long      | 273.0       | 1.91s                 | 22.23s    | ğŸŸ¢ Nhanh hÆ¡n 11.61x             |

### Tá»•ng káº¿t:

- `torch.compile` trung bÃ¬nh nhanh hÆ¡n **8.89 láº§n** so vá»›i ONNX Runtime.
- Trong táº¥t cáº£ cÃ¡c case thá»­ nghiá»‡m, `torch.compile` Ä‘á»u chiáº¿n tháº¯ng vá» tá»‘c Ä‘á»™.

| PhÆ°Æ¡ng phÃ¡p    | Tá»•ng lÆ°á»£t tháº¯ng | Tá»· lá»‡ tháº¯ng |
|----------------|------------------|--------------|
| ONNX           | 0                | âŒ 0.0%      |
| Compiled       | 3                | âœ… 100.0%    |

> âœ… **Recommendation:** DÃ¹ng `torch.compile` náº¿u má»¥c tiÃªu lÃ  inference tá»‘c Ä‘á»™ cao vá»›i model PyTorch.

---

## Khi nÃ o nÃªn dÃ¹ng ONNX thay vÃ¬ `torch.compile`?

DÃ¹ hiá»‡u nÄƒng ONNX khÃ´ng báº±ng compile trong thá»­ nghiá»‡m nÃ y, ONNX váº«n cÃ³ cÃ¡c lá»£i tháº¿:

| ONNX | `torch.compile` |
|------|-----------------|
| âœ… **Tá»‘t Ä‘á»ƒ chia sáº» hoáº·c deploy mÃ´ hÃ¬nh** sang cÃ¡c há»‡ thá»‘ng khÃ¡c (vÃ­ dá»¥: C++, Web, Mobile) | âŒ Phá»¥ thuá»™c PyTorch runtime |
| âœ… Cháº¡y Ä‘Æ°á»£c vá»›i ONNX Runtime â€“ nháº¹ hÆ¡n, dá»… tÃ­ch há»£p | âŒ Cáº§n toÃ n bá»™ PyTorch backend |
| âœ… Há»— trá»£ tá»‘t cho deployment cloud (Azure, Triton, etc) | âŒ KhÃ´ng tÆ°Æ¡ng thÃ­ch vá»›i má»i ná»n táº£ng |
| âŒ Hiá»‡u nÄƒng tháº¥p hÆ¡n vá»›i TTS | âœ… Tá»‘i Æ°u cao vá»›i tÃ­nh toÃ¡n thuáº§n tensor |
| âŒ KhÃ´ng linh hoáº¡t vá»›i cáº¥u trÃºc model phá»©c táº¡p (phá»¥ thuá»™c export) | âœ… Cháº¡y Ä‘Æ°á»£c cáº£ logic phá»©c táº¡p khÃ´ng cáº§n export |

---

## Nhá»¯ng váº¥n Ä‘á» thÆ°á»ng gáº·p khi dÃ¹ng ONNX

- â— **KhÃ´ng export Ä‘Æ°á»£c**: Má»™t sá»‘ model PyTorch dÃ¹ng logic phá»©c táº¡p (control flow, custom layers) khÃ´ng thá»ƒ export sang ONNX.
- â— **KhÃ¡c biá»‡t vá» káº¿t quáº£**: Do backend khÃ¡c nhau, ONNX cÃ³ thá»ƒ cho káº¿t quáº£ hÆ¡i khÃ¡c vá»›i PyTorch (Ä‘áº·c biá»‡t vá»›i float32 â†’ float16).
- â— **Cáº§n thÃªm bÆ°á»›c xá»­ lÃ½ dá»¯ liá»‡u**: CÃ¡c tensor Ä‘áº§u vÃ o cáº§n Ä‘Æ°á»£c convert sang `numpy`, khÃ¡c vá»›i workflow PyTorch.
---

## ğŸ™ Credits & Acknowledgements

<div align="center">

[![Style-Bert-VITS2](https://img.shields.io/badge/Style--Bert--VITS2-181717?style=for-the-badge&logo=github&logoColor=white)](https://github.com/litagin02/Style-Bert-VITS2)

Dá»± Ã¡n nÃ y dá»±a trÃªn Ã½ tÆ°á»Ÿng vÃ  mÃ£ nguá»“n cá»§a **StyleBertVits2** Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi [litagin02](https://github.com/litagin02).

</div>
