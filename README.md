# ƒê√¢y l√† project √°p d·ª•ng torch.compile v√† ONNX ƒë·ªÉ t·ªëi ∆∞u hi·ªáu su·∫•t c·ªßa m√¥ h√¨nh TTS.

> S·∫Ω c√≥ 2 ph·∫ßn 1 l√† v·ªÅ torch.compile, 2 l√† v·ªÅ ONNX v√† cu·ªëi c√πng l√† so s√°nh hi·ªáu nƒÉng c·ªßa c·∫£ 2.

# T·ªïng quan v·ªÅ `torch.compile`

## Tham s·ªë c·ªßa h√†m `torch.compile`

```python
torch.compile(
    model: Callable[[_InputT], _RetT],
    *,
    fullgraph: bool = False,
    dynamic: Optional[bool] = None,
    backend: Union[str, Callable] = 'inductor',
    mode: Optional[str] = None,
    options: Optional[Dict[str, Union[str, int, bool]]] = None,
    disable: bool = False
) ‚Üí Callable[[_InputT], _RetT]
```

### Gi·∫£i th√≠ch chi ti·∫øt c√°c tham s·ªë:

- **`model` (Callable)**  
  H√†m ho·∫∑c module m√† b·∫°n mu·ªën t·ªëi ∆∞u h√≥a b·∫±ng c√°ch bi√™n d·ªãch.

- **`fullgraph` (bool)**  
  - M·∫∑c ƒë·ªãnh l√† `False`: PyTorch s·∫Ω c·ªë g·∫Øng t√¨m c√°c ph·∫ßn trong model c√≥ th·ªÉ ƒë∆∞·ª£c bi√™n d·ªãch.  
  - N·∫øu ƒë·∫∑t l√† `True`: y√™u c·∫ßu to√†n b·ªô h√†m ph·∫£i ƒë∆∞·ª£c bi·ªÉu di·ªÖn th√†nh m·ªôt bi·ªÉu ƒë·ªì duy nh·∫•t. N·∫øu kh√¥ng th·ªÉ (c√≥ "graph breaks"), s·∫Ω b√°o l·ªói.

- **`dynamic` (bool ho·∫∑c None)**  
  - `True`: s·ª≠ d·ª•ng dynamic shape tracing ƒë·ªÉ tr√°nh ph·∫£i bi√™n d·ªãch l·∫°i khi k√≠ch th∆∞·ªõc ƒë·∫ßu v√†o thay ƒë·ªïi.  
  - `False`: lu√¥n lu√¥n chuy√™n bi·ªát h√≥a (specialize), kh√¥ng sinh kernel ƒë·ªông.  
  - `None` (m·∫∑c ƒë·ªãnh): t·ª± ƒë·ªông ph√°t hi·ªán thay ƒë·ªïi shape v√† bi√™n d·ªãch l·∫°i khi c·∫ßn thi·∫øt.

- **`backend` (str ho·∫∑c Callable)**  
  - Backend ƒë·ªÉ th·ª±c hi·ªán bi√™n d·ªãch. M·∫∑c ƒë·ªãnh l√† `"inductor"` ‚Äì c√¢n b·∫±ng t·ªët gi·ªØa hi·ªáu nƒÉng v√† chi ph√≠.  
  - D√πng `torch._dynamo.list_backends()` ƒë·ªÉ xem c√°c backend kh√¥ng th·ª≠ nghi·ªám.  
  - D√πng `torch._dynamo.list_backends(None)` ƒë·ªÉ xem c·∫£ backend th·ª≠ nghi·ªám/debug.  
  - C√≥ th·ªÉ ƒëƒÉng k√Ω backend tu·ª≥ ch·ªânh theo h∆∞·ªõng d·∫´n t·∫°i:  
    https://pytorch.org/docs/main/torch.compiler_custom_backends.html#registering-custom-backends

- **`mode` (str)**  
  M·ªôt s·ªë ch·∫ø ƒë·ªô c√≥ th·ªÉ s·ª≠ d·ª•ng:
  - `"default"`: ch·∫ø ƒë·ªô m·∫∑c ƒë·ªãnh, c√¢n b·∫±ng gi·ªØa hi·ªáu nƒÉng v√† overhead.
  - `"reduce-overhead"`: gi·∫£m chi ph√≠ v·∫≠n h√†nh Python, h·ªØu √≠ch v·ªõi batch nh·ªè, s·ª≠ d·ª•ng CUDA Graphs.
  - `"max-autotune"`: k√≠ch ho·∫°t t·ªëi ƒëa kh·∫£ nƒÉng autotuning (d√πng Triton, CUDA graphs n·∫øu c√≥).
  - `"max-autotune-no-cudagraphs"`: nh∆∞ tr√™n nh∆∞ng kh√¥ng s·ª≠ d·ª•ng CUDA graphs.

  C√≥ th·ªÉ d√πng `torch._inductor.list_mode_options()` ƒë·ªÉ xem c·ª• th·ªÉ c√°c c·∫•u h√¨nh t∆∞∆°ng ·ª©ng v·ªõi t·ª´ng mode.

- **`options` (dict)**  
  Tu·ª≥ ch·ªçn n√¢ng cao cho backend. M·ªôt v√†i option ƒë√°ng ch√∫ √Ω:
  - `epilogue_fusion`: g·ªôp c√°c ph√©p to√°n pointwise, c·∫ßn k·∫øt h·ª£p v·ªõi `max_autotune`.
  - `max_autotune`: cho ph√©p profiling ƒë·ªÉ ch·ªçn c·∫•u h√¨nh t·ªët nh·∫•t cho c√°c ph√©p nh√¢n ma tr·∫≠n.
  - `fallback_random`: h·ªó tr·ª£ debug c√°c v·∫•n ƒë·ªÅ v·ªÅ ƒë·ªô ch√≠nh x√°c.
  - `shape_padding`: pad th√™m k√≠ch th∆∞·ªõc tensor ƒë·ªÉ t·ªëi ∆∞u truy xu·∫•t b·ªô nh·ªõ GPU.
  - `triton.cudagraphs`: gi·∫£m overhead c·ªßa Python khi d√πng CUDA.
  - `trace.enabled`: flag h·ªó tr·ª£ debug hi·ªáu qu·∫£.
  - `trace.graph_diagram`: hi·ªÉn th·ªã s∆° ƒë·ªì bi·ªÉu ƒë·ªì sau khi th·ª±c hi·ªán fusion.

  ƒê·ªÉ xem danh s√°ch ƒë·∫ßy ƒë·ªß c√°c option h·ªó tr·ª£, d√πng `torch._inductor.list_options()`.

- **`disable` (bool)**  
  N·∫øu ƒë·∫∑t `True`, `torch.compile()` s·∫Ω kh√¥ng l√†m g√¨ c·∫£ ‚Äî ph√π h·ª£p khi c·∫ßn t·∫Øt compile t·∫°m th·ªùi ƒë·ªÉ test.

## V√≠ d·ª• s·ª≠ d·ª•ng

```python
@torch.compile(options={"triton.cudagraphs": True}, fullgraph=True)
def foo(x):
    return torch.sin(x) + torch.cos(x)
```

## M·ªôt v√†i l∆∞u √Ω sau khi th·ª≠ nghi·ªám th·ª±c t·∫ø

Theo nh∆∞ testing em ƒë√£ l√†m th√¨ vi·ªác s·ª≠ d·ª•ng `torch.compile` c·∫ßn ƒë∆∞·ª£c c√¢n nh·∫Øc k·ªπ, kh√¥ng th·ªÉ "compile" m·ªôt model m·ªôt c√°ch m√°y m√≥c.  

C·ª• th·ªÉ:
- `torch.compile` ch·ªâ ho·∫°t ƒë·ªông hi·ªáu qu·∫£ v·ªõi ph·∫ßn **t√≠nh to√°n thu·∫ßn t√∫y**.
- Nh·ªØng ph·∫ßn li√™n quan ƒë·∫øn I/O (v√≠ d·ª•: ƒë·ªçc file, ghi log, print, ho·∫∑c c√°c thao t√°c kh√¥ng thu·∫ßn tensor) c·∫ßn ƒë∆∞·ª£c t√°ch **ra kh·ªèi h√†m `forward`** ho·∫∑c kh√¥ng n√™n n·∫±m trong ph·∫°m vi bi√™n d·ªãch.
- Khi s·ª≠ d·ª•ng sai c√°ch, PyTorch s·∫Ω kh√¥ng th·ªÉ d·ª±ng ƒë∆∞·ª£c bi·ªÉu ƒë·ªì ho·∫∑c hi·ªáu nƒÉng s·∫Ω kh√¥ng nh∆∞ mong ƒë·ª£i.

## Li·ªáu s·ª≠ d·ª•ng `torch.compile` c√≥ guarantee v·ªÅ vi·ªác improve performance khi s·ª≠ d·ª•ng?

C√¢u tr·∫£ l·ªùi l√† **c√≥** v√† **kh√¥ng**.

C·ª• th·ªÉ th√¨ v·ªõi **single batch inference**, c√≥ nh·ªØng tr∆∞·ªùng h·ª£p s·∫Ω ch·∫°y nhanh h∆°n, nh∆∞ng c≈©ng c√≥ nh·ªØng tr∆∞·ªùng h·ª£p hi·ªáu nƒÉng s·∫Ω k√©m h∆°n. Tuy nhi√™n, v·ªõi **multi-batch inference** (inference nhi·ªÅu l·∫ßn v·ªõi c√πng m·ªôt model), theo k·∫øt qu·∫£ th·ª≠ nghi·ªám c·ªßa em th√¨ **√≠t nhi·ªÅu s·∫Ω c√≥ c·∫£i thi·ªán hi·ªáu nƒÉng sau khi compile**. ƒêi·ªÅu n√†y c√≥ th·ªÉ l√Ω gi·∫£i l√† do c∆° ch·∫ø caching v√† t·ªëi ∆∞u ho√° kernel ch·ªâ ph√°t huy hi·ªáu qu·∫£ sau m·ªôt v√†i l·∫ßn ch·∫°y ƒë·∫ßu ti√™n.

## T·∫°i sao v·ªõi m√¥ h√¨nh TTS ch·ªâ c√≥ th·ªÉ compile v√†o `g_net` thay v√¨ compile nh∆∞ c√°ch em ƒë√£ l√†m?

Sau khi t√¨m hi·ªÉu l·∫°i th√¨ em nh·∫≠n ra m√¨nh ƒë√£ s·ª≠ d·ª•ng `torch.compile` ch∆∞a ƒë√∫ng c√°ch. D√π v·ªÅ m·∫∑t k·ªπ thu·∫≠t th√¨ vi·ªác compile to√†n b·ªô m√¥ h√¨nh v·∫´n ch·∫°y ƒë∆∞·ª£c, nh∆∞ng hi·ªáu nƒÉng th·ª±c t·∫ø l·∫°i **kh√¥ng ·ªïn ƒë·ªãnh**, v√† **ƒëa s·ªë tr∆∞·ªùng h·ª£p c√≤n ch·∫≠m h∆°n c·∫£ khi kh√¥ng d√πng compile**.

L√Ω do ch√≠nh l√†:
- Trong m√¥ h√¨nh TTS, kh√¥ng ph·∫£i to√†n b·ªô pipeline ƒë·ªÅu mang t√≠nh "t√≠nh to√°n thu·∫ßn tensor" (pure tensor computation). C√≥ r·∫•t nhi·ªÅu thao t√°c li√™n quan ƒë·∫øn I/O (nh∆∞ x·ª≠ l√Ω vƒÉn b·∫£n, thao t√°c v·ªõi waveform, ti·ªÅn x·ª≠ l√Ω ho·∫∑c h·∫≠u x·ª≠ l√Ω), nh·ªØng ph·∫ßn n√†y **kh√¥ng ph√π h·ª£p ƒë·ªÉ compile**, ho·∫∑c **l√†m gi√°n ƒëo·∫°n graph**, d·∫´n ƒë·∫øn m·∫•t t·ªëi ∆∞u.
- Khi compile to√†n b·ªô pipeline, PyTorch s·∫Ω kh√≥ t·ªëi ∆∞u do c√≥ qu√° nhi·ªÅu ph·∫ßn kh√¥ng th·ªÉ bi·ªÉu di·ªÖn th√†nh graph, d·∫´n ƒë·∫øn vi·ªác t·∫°o ra nhi·ªÅu **"graph break"** v√† **recompilation kh√¥ng c·∫ßn thi·∫øt**.
- Trong khi ƒë√≥, `g_net` l√† ph·∫ßn model t√≠nh to√°n ch√≠nh, th∆∞·ªùng l√† m·ªôt m·∫°ng neural network thu·∫ßn tu√Ω. Vi·ªác compile ri√™ng ph·∫ßn n√†y s·∫Ω:
  - Gi·∫£m chi ph√≠ tracing l·∫°i graph.
  - T·∫≠n d·ª•ng t·ªët nh·∫•t kh·∫£ nƒÉng t·ªëi ∆∞u kernel.
  - Kh√¥ng b·ªã ·∫£nh h∆∞·ªüng b·ªüi c√°c thao t√°c I/O kh√°c.

=> V√¨ v·∫≠y, **vi·ªác compile ch·ªâ ri√™ng `g_net` l√† c√°ch t·ªëi ∆∞u nh·∫•t**, ƒë·∫£m b·∫£o t·∫≠n d·ª•ng ƒë∆∞·ª£c hi·ªáu nƒÉng c·ªßa `torch.compile` m√† kh√¥ng ·∫£nh h∆∞·ªüng ƒë·∫øn ph·∫ßn c√≤n l·∫°i c·ªßa pipeline.

## S·ª± kh√°c bi·ªát gi·ªØa **mode** v√† **backend** trong `torch.compile()`

Hai tham s·ªë `mode` v√† `backend` ƒë·ªÅu ·∫£nh h∆∞·ªüng ƒë·∫øn c√°ch PyTorch t·ªëi ∆∞u m√¥ h√¨nh khi s·ª≠ d·ª•ng `torch.compile`, nh∆∞ng vai tr√≤ v√† m·ª©c ƒë·ªô ki·ªÉm so√°t c·ªßa ch√∫ng l√† **kh√°c nhau**.

### 1. `backend` ‚Äì *Tham s·ªë n√†y l√† ki·ªÉu l·ª±a ch·ªçn b·ªô m√°y ƒë·ªÉ bi√™n d·ªãch code*

- ƒê√¢y l√† th√†nh ph·∫ßn ch√≠nh x·ª≠ l√Ω vi·ªác **chuy·ªÉn ƒë·ªïi model sang d·∫°ng t·ªëi ∆∞u h√≥a** v√† th·ª±c thi ch√∫ng.
- Hi·ªÉu ƒë∆°n gi·∫£n th√¨ `backend` gi·ªëng nh∆∞ **"c√¥ng c·ª•" ho·∫∑c "engine"** ƒë·ª©ng sau vi·ªác bi√™n d·ªãch v√† th·ª±c thi m√¥ h√¨nh.
- M·∫∑c ƒë·ªãnh l√† `"inductor"` ‚Äì m·ªôt backend do PyTorch ph√°t tri·ªÉn, t·ªëi ∆∞u t·ªët cho GPU/CPU.
- C√≥ th·ªÉ thay b·∫±ng c√°c backend kh√°c (v√≠ d·ª•: `"onnxrt"` ho·∫∑c backend t√πy ch·ªânh).

V√≠ d·ª•:
```python
torch.compile(model, backend='inductor')
```

> V·ªõi th·ª≠ nghi·ªám c·ªßa em th√¨ vi·ªác thay ƒë·ªïi gi·ªØa c√°c backend em kh√¥ng th·ª±c s·ª± nh·∫≠n th·∫•y s·ª± kh√°c nhau l·∫Øm (c√≥ th·ªÉ l√† v√¨ ch·ªâ th·ª±c hi·ªán infer tr√™n 7 samples)

### 2. `mode` ‚Äì *H∆∞·ªõng t·ªëi ∆∞u*

- `mode` x√°c ƒë·ªãnh **chi·∫øn l∆∞·ª£c t·ªëi ∆∞u h√≥a** ƒë∆∞·ª£c √°p d·ª•ng trong qu√° tr√¨nh compile.
- ƒê√¢y gi·ªëng nh∆∞ **"preset c·∫•u h√¨nh"** cho backend ‚Äì gi√∫p b·∫°n ch·ªçn gi·ªØa vi·ªác ∆∞u ti√™n t·ªëc ƒë·ªô, b·ªô nh·ªõ, hay autotuning.
- M·ªói mode s·∫Ω set c√°c flag n·ªôi b·ªô kh√°c nhau trong backend (th∆∞·ªùng l√† inductor).

C√°c mode ph·ªï bi·∫øn:
- `"default"`: c√¢n b·∫±ng hi·ªáu nƒÉng v√† ƒë·ªô ·ªïn ƒë·ªãnh, ph√π h·ª£p cho h·∫ßu h·∫øt tr∆∞·ªùng h·ª£p.
- `"reduce-overhead"`: gi·∫£m chi ph√≠ th·ª±c thi Python, th√≠ch h·ª£p cho batch nh·ªè (d√πng CUDA graphs).
- `"max-autotune"`: b·∫≠t autotune t·ªëi ƒëa, t√¨m c·∫•u h√¨nh t·ªët nh·∫•t cho c√°c ph√©p to√°n nh∆∞ matmul/convolution.
- `"max-autotune-no-cudagraphs"`: nh∆∞ tr√™n, nh∆∞ng kh√¥ng d√πng CUDA graphs (ph√π h·ª£p v·ªõi m·ªôt s·ªë constraint ƒë·∫∑c bi·ªát).

V√≠ d·ª•:
```python
torch.compile(model, mode='max-autotune')
```

> T∆∞∆°ng t·ª± nh∆∞ `backend`, em kh√¥ng th·∫•y c√≥ s·ª± kh√°c nhau r√µ r√†ng

## V·ªÅ vi·ªác th·ª≠ nghi·ªám compile xong hi·ªáu nƒÉng c√≤n k√©m h∆°n

Vi·ªác n√†y "ƒëi√™u" th·∫≠t, ch√≠nh v√¨ em ƒë√£ c√≥ gi·∫£i th√≠ch ·ªü tr√™n, ƒë√≥ l√† em l√†m sai

## bert-vits chuy·ªÉn t·ª´ `.pt` sang `.safetensors`, l√Ω do l√† g√¨?

Theo nh∆∞ research c·ªßa em th√¨ l√Ω do ch√≠nh n·∫±m ·ªü **t√≠nh an to√†n v√† t·ªëc ƒë·ªô t·∫£i m√¥ h√¨nh**.

### 1. `.safetensors` gi√∫p **tr√°nh c√°c v·∫•n ƒë·ªÅ b·∫£o m·∫≠t**:

- File `.pt` (ho·∫∑c `.pth`) th∆∞·ªùng ƒë∆∞·ª£c l∆∞u b·∫±ng `torch.save()`, m√† b·∫£n ch·∫•t l√† s·ª≠ d·ª•ng `pickle` ‚Äì c∆° ch·∫ø n√†y cho ph√©p th·ª±c thi **m√£ Python t√πy √Ω** khi load l·∫°i model (`torch.load()`).
- ƒêi·ªÅu n√†y ti·ªÅm ·∫©n r·ªßi ro b·∫£o m·∫≠t, ƒë·∫∑c bi·ªát khi load m√¥ h√¨nh t·ª´ ngu·ªìn kh√¥ng tin c·∫≠y: ai ƒë√≥ c√≥ th·ªÉ nh√∫ng m√£ ƒë·ªôc v√† n√≥ s·∫Ω **t·ª± ƒë·ªông ch·∫°y khi load m√¥ h√¨nh**.
- Trong khi ƒë√≥, `.safetensors` l√† ƒë·ªãnh d·∫°ng **ho√†n to√†n kh√¥ng th·ª±c thi code**, ch·ªâ l∆∞u d·ªØ li·ªáu tensor thu·∫ßn t√∫y, v√¨ v·∫≠y lo·∫°i b·ªè kh·∫£ nƒÉng ch√®n m√£ ƒë·ªôc.

### 2. `.safetensors` c√≥ t·ªëc ƒë·ªô load **nhanh h∆°n**:

- File `.safetensors` ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ **cho ph√©p memory mapping** v√† truy c·∫≠p song song, gi√∫p tƒÉng t·ªëc ƒë·ªô load m√¥ h√¨nh, ƒë·∫∑c bi·ªát l√† khi d√πng tr√™n GPU.
- C·∫•u tr√∫c c·ªßa ƒë·ªãnh d·∫°ng n√†y gi√∫p tr√°nh vi·ªác gi·∫£i n√©n t·ª´ng ph·∫ßn nh∆∞ `.pt`, do ƒë√≥ ti·∫øt ki·ªám th·ªùi gian kh·ªüi t·∫°o model.

### 3. H·ªó tr·ª£ r·ªông trong c·ªông ƒë·ªìng hi·ªán t·∫°i:

- C√°c d·ª± √°n l·ªõn nh∆∞ Hugging Face Transformers ƒë√£ **m·∫∑c ƒë·ªãnh h·ªó tr·ª£ v√† khuy·∫øn ngh·ªã d√πng `.safetensors`** cho c√°c model pre-trained.
- V·ªõi update g·∫ßn ƒë√¢y c·ªßa Pytoch (2.6.0 onward) th√¨ khi s·ª≠ d·ª•ng `torch.load()`, tham s·ªë `weights_only` ƒë√£ ƒë∆∞·ª£c set th√†nh **True** m·∫∑c ƒë·ªãnh thay v√¨ **False** (load c·∫£ code) nh∆∞ tr∆∞·ªõc ƒë√¢y.
- V·ªõi c√°c m√¥ h√¨nh nh∆∞ `bert-vits`, vi·ªác chuy·ªÉn sang `.safetensors` c≈©ng gi√∫p ƒë·ªìng b·ªô t·ªët h∆°n v·ªõi c√°c pipeline hi·ªán ƒë·∫°i, d·ªÖ d√†ng chia s·∫ª m√† v·∫´n ƒë·∫£m b·∫£o an to√†n.

## L∆∞u √Ω khi s·ª≠ d·ª•ng `torch.compile()`

### `torch.compile()` **ch·ªâ h·ªó tr·ª£ GPU c√≥ compute capability ‚â• 7.0**

- `torch.compile()` t·∫≠n d·ª•ng c√°c c√¥ng ngh·ªá nh∆∞ **Triton**, **Inductor**, v√† c√°c k·ªπ thu·∫≠t kernel fusion hi·ªán ƒë·∫°i, v·ªën ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ ho·∫°t ƒë·ªông hi·ªáu qu·∫£ nh·∫•t tr√™n ki·∫øn tr√∫c GPU t·ª´ **Volta** tr·ªü l√™n (t·ª©c l√† compute capability ‚â• 7.0).
- N·∫øu d√πng GPU v·ªõi compute capability th·∫•p h∆°n s·∫Ω kh√¥ng th·ªÉ compile.

### C√°ch ki·ªÉm tra compute capability c·ªßa GPU:

C√≥ th·ªÉ ch·∫°y l·ªánh sau trong Python:

```python
import torch
print(torch.cuda.get_device_name(0))
print(torch.cuda.get_device_capability(0))
```


# T·ªïng quan v·ªÅ **ONNX**

## ONNX l√† g√¨?

**ONNX** (Open Neural Network Exchange) l√† m·ªôt ƒë·ªãnh d·∫°ng trung gian d√πng ƒë·ªÉ **xu·∫•t v√† chia s·∫ª m√¥ h√¨nh h·ªçc s√¢u** gi·ªØa c√°c framework kh√°c nhau (v√≠ d·ª•: PyTorch, TensorFlow, ONNX Runtime, v.v).

N√≥ cho ph√©p b·∫°n **xu·∫•t model t·ª´ PyTorch**, sau ƒë√≥ **ch·∫°y inference b·∫±ng ONNX Runtime**, th∆∞·ªùng s·∫Ω t·ªëi ∆∞u h√≥a hi·ªáu su·∫•t h∆°n nh·ªù engine inference nh·∫π, ƒë·∫∑c bi·ªát trong m√¥i tr∆∞·ªùng production ho·∫∑c mobile.

---

## C√°c b∆∞·ªõc ch√≠nh ƒë·ªÉ s·ª≠ d·ª•ng ONNX trong PyTorch

### 1. Export m√¥ h√¨nh sang ONNX

```python
torch.onnx.export(
    model,                      # M√¥ h√¨nh PyTorch
    sample_input,               # M·ªôt batch input m·∫´u ƒë·ªÉ trace
    "model.onnx",               # T√™n file ƒë·∫ßu ra
    input_names=["input"],      # (Optional) T√™n tensor ƒë·∫ßu v√†o
    output_names=["output"],    # (Optional) T√™n tensor ƒë·∫ßu ra
    dynamic_axes={              # (Optional) Cho ph√©p k√≠ch th∆∞·ªõc ƒë·ªông
        "input": {0: "batch_size"},
        "output": {0: "batch_size"}
    },
    opset_version=17,           # Phi√™n b·∫£n opset ONNX
    do_constant_folding=True,   # T·ªëi ∆∞u bi·ªÉu th·ª©c h·∫±ng
    export_params=True          # L∆∞u tr·ªçng s·ªë v√†o file ONNX
)
```

> üéØ **L∆∞u √Ω:** `opset_version` n√™n l√† 17 tr·ªü l√™n ƒë·ªÉ ƒë·∫£m b·∫£o t∆∞∆°ng th√≠ch t·ªët v·ªõi ONNX Runtime m·ªõi.

#### Ho·∫∑c c√≥ th·ªÉ d√πng Optimum-CLI nh∆∞ v√≠ d·ª• sau ƒë√¢y

```bash
pip install optimum[exporters]
```
Set --model d√πng ƒë·ªÉ export model c·ªßa Pytorch ho·∫∑c TensorFlow
```bash
optimum-cli export onnx --model distilbert/distilbert-base-uncased-distilled-squad distilbert_base_uncased_squad_onnx/
```
---

### 2. Load v√† ch·∫°y inference b·∫±ng ONNX Runtime

```python
import onnxruntime as ort
import numpy as np

session = ort.InferenceSession("model.onnx")
inputs = {"input": input_tensor.numpy()}
outputs = session.run(None, inputs)
```

### 3. L∆∞u √Ω v·ªÅ s·ª± kh√°c bi·ªát gi·ªØa **ONNX** v√† **torch.compile()** khi compile

V·ªõi m√¥ h√¨nh TTS, n·∫øu d√πng **ONNX**, **kh√¥ng th·ªÉ export c·∫£ model m·ªôt c√°ch tr·ª±c ti·∫øp**. V·∫≠y n√™n th∆∞·ªùng ph·∫£i **chia nh·ªè ra**, export ri√™ng t·ª´ng ph·∫ßn nh∆∞ `encode`, `decode`,... th√¨ m·ªõi ch·∫°y ƒë∆∞·ª£c ‚Äî kh√° phi·ªÅn v√† d·ªÖ ph√°t sinh l·ªói.

Trong khi ƒë√≥, v·ªõi **`torch.compile()`**, ch·ªâ c·∫ßn **t·∫°o m·ªôt wrapper cho `forward()`** ch·ª©a ph·∫ßn t√≠nh to√°n ch√≠nh, mi·ªÖn l√† kh√¥ng c√≥ thao t√°c I/O ho·∫∑c logic ph·ª©c t·∫°p. V·∫≠y l√† xong, compile ƒë∆∞·ª£c ngay. **D·ªÖ h∆°n** so v·ªõi vi·ªác ph·∫£i chia nh·ªè ƒë·ªÉ export ONNX.

---

## So s√°nh hi·ªáu nƒÉng: `torch.compile` vs ONNX

D∆∞·ªõi ƒë√¢y l√† k·∫øt qu·∫£ th·ª≠ nghi·ªám v·ªõi m√¥ h√¨nh Text-to-Speech (TTS), ƒë∆∞·ª£c ƒëo theo ƒë·ªô d√†i vƒÉn b·∫£n:

| ƒê·ªô d√†i vƒÉn b·∫£n | S·ªë k√Ω t·ª± TB | `torch.compile` Time | ONNX Time | T·ªëc ƒë·ªô (`compiled` vs `ONNX`) |
|----------------|-------------|-----------------------|-----------|-------------------------------|
| Very Short     | 3.0         | 0.17s                 | 0.33s     | üü¢ `torch.compile` nhanh h∆°n 1.89x |
| Short          | 12.2        | 0.18s                 | 0.82s     | üü¢ Nhanh h∆°n 4.61x              |
| Medium         | 34.0        | 0.26s                 | 2.10s     | üü¢ Nhanh h∆°n 7.98x              |
| Long           | 91.7        | 0.61s                 | 6.15s     | üü¢ Nhanh h∆°n 10.12x             |
| Very Long      | 273.0       | 1.91s                 | 22.23s    | üü¢ Nhanh h∆°n 11.61x             |

### T·ªïng k·∫øt:

- `torch.compile` trung b√¨nh nhanh h∆°n **8.89 l·∫ßn** so v·ªõi ONNX Runtime.
- Trong t·∫•t c·∫£ c√°c case th·ª≠ nghi·ªám, `torch.compile` ƒë·ªÅu chi·∫øn th·∫Øng v·ªÅ t·ªëc ƒë·ªô.

| Ph∆∞∆°ng ph√°p    | T·ªïng l∆∞·ª£t th·∫Øng | T·ª∑ l·ªá th·∫Øng |
|----------------|------------------|--------------|
| ONNX           | 0                | ‚ùå 0.0%      |
| Compiled       | 3                | ‚úÖ 100.0%    |

> ‚úÖ **Recommendation:** D√πng `torch.compile` n·∫øu m·ª•c ti√™u l√† inference t·ªëc ƒë·ªô cao v·ªõi model PyTorch.

---

## Khi n√†o n√™n d√πng ONNX thay v√¨ `torch.compile`?

D√π hi·ªáu nƒÉng ONNX kh√¥ng b·∫±ng compile trong th·ª≠ nghi·ªám n√†y, ONNX v·∫´n c√≥ c√°c l·ª£i th·∫ø:

| ONNX | `torch.compile` |
|------|-----------------|
| ‚úÖ **T·ªët ƒë·ªÉ chia s·∫ª ho·∫∑c deploy m√¥ h√¨nh** sang c√°c h·ªá th·ªëng kh√°c (v√≠ d·ª•: C++, Web, Mobile) | ‚ùå Ph·ª• thu·ªôc PyTorch runtime |
| ‚úÖ Ch·∫°y ƒë∆∞·ª£c v·ªõi ONNX Runtime ‚Äì nh·∫π h∆°n, d·ªÖ t√≠ch h·ª£p | ‚ùå C·∫ßn to√†n b·ªô PyTorch backend |
| ‚úÖ H·ªó tr·ª£ t·ªët cho deployment cloud (Azure, Triton, etc) | ‚ùå Kh√¥ng t∆∞∆°ng th√≠ch v·ªõi m·ªçi n·ªÅn t·∫£ng |
| ‚ùå Hi·ªáu nƒÉng th·∫•p h∆°n v·ªõi TTS | ‚úÖ T·ªëi ∆∞u cao v·ªõi t√≠nh to√°n thu·∫ßn tensor |
| ‚ùå Kh√¥ng linh ho·∫°t v·ªõi c·∫•u tr√∫c model ph·ª©c t·∫°p (ph·ª• thu·ªôc export) | ‚úÖ Ch·∫°y ƒë∆∞·ª£c c·∫£ logic ph·ª©c t·∫°p kh√¥ng c·∫ßn export |

---

## Nh·ªØng v·∫•n ƒë·ªÅ th∆∞·ªùng g·∫∑p khi d√πng ONNX

- ‚ùó **Kh√¥ng export ƒë∆∞·ª£c**: M·ªôt s·ªë model PyTorch d√πng logic ph·ª©c t·∫°p (control flow, custom layers) kh√¥ng th·ªÉ export sang ONNX.
- ‚ùó **Kh√°c bi·ªát v·ªÅ k·∫øt qu·∫£**: Do backend kh√°c nhau, ONNX c√≥ th·ªÉ cho k·∫øt qu·∫£ h∆°i kh√°c v·ªõi PyTorch (ƒë·∫∑c bi·ªát v·ªõi float32 ‚Üí float16).
- ‚ùó **C·∫ßn th√™m b∆∞·ªõc x·ª≠ l√Ω d·ªØ li·ªáu**: C√°c tensor ƒë·∫ßu v√†o c·∫ßn ƒë∆∞·ª£c convert sang `numpy`, kh√°c v·ªõi workflow PyTorch.
---